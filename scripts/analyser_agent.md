# SIMPLE RESULT ANALYSIS SYSTEM

## üéØ CORE OBJECTIVE
Create a **self-contained, simple system** that can verify if script execution results are reliable using **ONLY available data** (execution.txt and report.html).

## üÜï UPDATED SKILL NAMES
- `validate` - Core validation skill (was: ultra-simple-validation)
- `analyze` - Failure analysis skill (was: ultra-simple-failure-analysis)

**Why shorter names?**
- ‚úÖ Easier to remember and use
- ‚úÖ Cleaner configuration
- ‚úÖ More intuitive for CLI usage
- ‚úÖ Follows verb-noun pattern

## üöÄ KEY PRINCIPLES

1. **SELF-CONTAINED**: Use only execution.txt and report.html - no test case loading
2. **SIMPLE RULES**: Easy-to-understand validation logic
3. **BINARY DECISIONS**: RELIABLE or UNRELIABLE
4. **CLEAR CLASSIFICATION**: BUG, SCRIPT_ISSUE, SYSTEM_ISSUE, or UNKNOWN
5. **EVIDENCE-BASED**: Provide clear reasoning from available data

## üîç VALIDATION RULES

### For ALL Results (PASS or FAIL)

#### ‚úÖ INITIAL STATE CHECK
```
- Look at initial screenshot description in execution.txt
- Check for keywords: "black screen", "no signal", "error", "disconnected"
- Verify device appears normal and responsive
- If any issues found ‚Üí UNRELIABLE
```

#### ‚úÖ FINAL STATE CHECK
```
- Look at final screenshot description in execution.txt  
- Check for keywords: "black screen", "no signal", "error", "frozen"
- Verify device appears normal and responsive
- If any issues found ‚Üí UNRELIABLE
```

### For PASS Results

#### ‚úÖ RESULT COHERENCE CHECK
```
- Verify final state matches expected test outcome (from execution.txt)
- Check no error messages or warnings visible in final screenshot
- Confirm device state looks correct for test goal
- If inconsistent ‚Üí UNRELIABLE_PASS
```

### For FAIL Results

#### ‚úÖ FAILURE ANALYSIS
```
- Look at final screenshot: What does it actually show?
- Check execution.txt: Where exactly did it fail?
- Compare with previous steps: What changed?
- Apply simple classification rules
```

## ü§ñ DECISION LOGIC

### PASS Results
```
IF (initial_state_ok AND final_state_ok AND result_coherent)
    ‚Üí RELIABLE_PASS (can trust this result)
ELSE
    ‚Üí UNRELIABLE_PASS (needs manual review)
```

### FAIL Results
```
IF (initial_state_ok AND final_state_ok)
    ‚Üí Apply failure classification rules
ELSE
    ‚Üí UNRELIABLE_FAILURE (missing critical data)
```

## üéØ FAILURE CLASSIFICATION (Simple Rules)

### Using ONLY data from execution.txt and report.html:

#### üîç STEP 1: Examine Final Screenshot
```
- What elements are visible?
- Any error messages displayed?
- Device state: normal/black/frozen/error?
```

#### üîç STEP 2: Find Failure Details
```
- Which step failed? (from execution.txt)
- What error message? (exact text)
- What was expected vs actual?
```

#### üîç STEP 3: Check Previous Steps
```
- What actions were performed before failure?
- What was the execution sequence?
- Any patterns or clues about what went wrong?
```

#### ‚úÖ STEP 4: Apply Simple Classification

**RULE 1: BUG (Real Device Issue)**
```
IF (screenshot shows element BUT error says "not found")
    ‚Üí CLASSIFICATION: BUG
    ‚Üí REASONING: "Element visible in screenshot but test reports not found"
    ‚Üí CONFIDENCE: HIGH
```

**RULE 2: SCRIPT_ISSUE (Test Problem)**
```
IF (error mentions selector/timing/expected value/wait)
    ‚Üí CLASSIFICATION: SCRIPT_ISSUE
    ‚Üí REASONING: "Test implementation issue - selector, timing, or expectation problem"
    ‚Üí CONFIDENCE: MEDIUM
```

**RULE 3: SYSTEM_ISSUE (Infrastructure Problem)**
```
IF (screenshot shows black screen/no signal/device disconnected)
    ‚Üí CLASSIFICATION: SYSTEM_ISSUE
    ‚Üí REASONING: "Device or connection problem - black screen, no signal, or disconnect"
    ‚Üí CONFIDENCE: HIGH
```

**RULE 4: UNKNOWN (Need More Data)**
```
IF (unclear from available data OR conflicting evidence)
    ‚Üí CLASSIFICATION: UNKNOWN
    ‚Üí REASONING: "Cannot determine from available data - needs manual review"
    ‚Üí CONFIDENCE: LOW
```

## üìã OUTPUT FORMAT

### For RELIABLE Results
```yaml
analysis_result:
  status: RELIABLE
  confidence: HIGH
  validation:
    initial_state: OK
    final_state: OK
    result_coherence: OK
  evidence:
    - Initial screenshot: [description from execution.txt]
    - Final screenshot: [description from execution.txt]
    - Execution flow: [summary from execution.txt]
  recommendation: TRUST_RESULT
```

### For UNRELIABLE Results
```yaml
analysis_result:
  status: UNRELIABLE
  confidence: MEDIUM|LOW
  classification: BUG|SCRIPT_ISSUE|SYSTEM_ISSUE|UNKNOWN
  validation:
    initial_state: OK|FAIL
    final_state: OK|FAIL
    result_coherence: OK|FAIL|N/A
  evidence:
    - Final screenshot shows: [description]
    - Error message: [exact text]
    - Failed step: [step name]
    - Previous actions: [summary]
  reasoning: [detailed reasoning using simple rules]
  recommendation: REVIEW_MANUALLY|DISCARD_RESULT
```

## üõ†Ô∏è IMPLEMENTATION PLAN

### Phase 1: Core Validation Skill
```yaml
name: validate
version: 1.0.0
description: Self-contained result validation using only available data

system_prompt: |
  ULTRA SIMPLE VALIDATION - Use ONLY execution.txt and report.html
  
  CHECKLIST:
  1. Initial state OK? (no black screen, no signal issues, device responsive)
  2. Final state OK? (no black screen, no errors, device responsive)
  3. For PASS: Result coherent? (final state matches test goal)
  
  DECISION:
  - All checks OK ‚Üí RELIABLE
  - Any check fails ‚Üí UNRELIABLE

tools:
  - read_execution_logs
  - parse_report_data
  - check_screenshot_descriptions
```

### Phase 2: Simple Failure Analyzer
```yaml
name: analyze
version: 1.0.0
description: Self-contained failure analysis using simple rules

system_prompt: |
  ULTRA SIMPLE FAILURE ANALYSIS - Use ONLY available data
  
  SIMPLE RULES:
  1. If element visible but "not found" error ‚Üí BUG
  2. If selector/timing/expectation error ‚Üí SCRIPT_ISSUE
  3. If black screen/no signal ‚Üí SYSTEM_ISSUE
  4. If unclear ‚Üí UNKNOWN
  
  Always provide clear evidence and reasoning!

tools:
  - read_execution_logs
  - parse_report_data
  - extract_screenshot_info
  - find_failure_details
```

### Phase 3: Update Analyzer Configuration
```yaml
# In analyzer.yaml
available_skills:
  - validate
  - analyze
  - generate-simple-report
```

## üéØ KEY BENEFITS

‚úÖ **Self-contained** - Uses only data we actually have
‚úÖ **Simple rules** - Easy to understand and maintain
‚úÖ **Clear decisions** - Binary reliable/unreliable output
‚úÖ **Actionable** - Clear recommendations (TRUST/REVIEW/DISCARD)
‚úÖ **No dependencies** - Doesn't require test case loading
‚úÖ **Fast to implement** - Can be built and tested quickly
‚úÖ **Easy to enhance** - Simple foundation for future improvements
‚úÖ **Short names** - `validate` and `analyze` are clean and intuitive

## üß™ TESTING APPROACH

1. **Test with real execution.txt files**
2. **Verify simple rules work correctly**
3. **Check output format is clear**
4. **Validate classification accuracy**
5. **Ensure self-contained operation**

## üìà FUTURE ENHANCEMENTS

- Add more sophisticated screenshot analysis
- Include timing analysis
- Add historical data comparison
- Implement confidence scoring
- Add automated learning from manual reviews

## üéØ FINAL SUMMARY

**What we've created**:
- `validate` skill: Simple binary reliability check
- `analyze` skill: Detailed failure classification  
- Clean, short names that are easy to use
- Self-contained system using only available data
- Simple rules anyone can understand

**Files created**:
- `validate.yaml` - Core validation skill
- `analyze.yaml` - Failure analysis skill
- Updated `analyzer.yaml` with new skills
- Comprehensive documentation

This simple system provides a **working, reliable foundation** that solves the core problem: preventing unreliable results from being trusted, using only the data we actually have available.