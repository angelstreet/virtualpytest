"""
Standardized Verification Executor

This module provides a standardized way to execute verifications that can be used by:
- Python code directly (navigation execution, scripts, etc.)
- API endpoints (maintaining consistency)
- Frontend hooks (via API calls)

The core logic is the same as /server/verification/executeBatch but available as a reusable class.
"""

import time
import io
import sys
from typing import Dict, List, Optional, Any, Tuple
from shared.src.lib.database.execution_results_db import record_node_execution


class VerificationExecutor:
    """
    Standardized verification executor that provides consistent verification execution
    across Python code and API endpoints.
    
    CRITICAL: Do not create new instances directly! Use device.verification_executor instead.
    Each device has a singleton VerificationExecutor that preserves navigation context.
    """
    
    @classmethod
    def get_for_device(cls, device):
        """
        Factory method to get the device's existing VerificationExecutor.
        
        RECOMMENDED: Use device.verification_executor directly instead of this method.
        
        Args:
            device: Device instance
            
        Returns:
            The device's existing VerificationExecutor instance
            
        Raises:
            ValueError: If device doesn't have a verification_executor
        """
        if not hasattr(device, 'verification_executor') or not device.verification_executor:
            raise ValueError(f"Device {device.device_id} does not have a VerificationExecutor. "
                           "VerificationExecutors are created during device initialization.")
        return device.verification_executor
    
    def __init__(self, device, tree_id: str = None, node_id: str = None, _from_device_init: bool = False):
        """
        Initialize VerificationExecutor
        
        Args:
            device: Device instance (mandatory, contains host_name and device_id)
            tree_id: Tree ID for navigation context
            node_id: Node ID for navigation context
            _from_device_init: Internal flag to indicate creation from device initialization
        """
        if not device:
            raise ValueError("Device instance is required")
        if not device.host_name:
            raise ValueError("Device must have host_name")
        if not device.device_id:
            raise ValueError("Device must have device_id")
        
        if not _from_device_init:
            import traceback
            print(f"⚠️ [VerificationExecutor] WARNING: Creating new VerificationExecutor instance for device {device.device_id}")
            print(f"⚠️ [VerificationExecutor] This may cause state loss! Use device.verification_executor instead.")
            print(f"⚠️ [VerificationExecutor] Call stack:")
            for line in traceback.format_stack()[-3:-1]:  # Show last 2 stack frames
                print(f"⚠️ [VerificationExecutor]   {line.strip()}")
        
        # Store instances directly
        self.device = device
        self.host_name = device.host_name
        self.device_id = device.device_id
        self.device_model = device.device_model
        self.device_name = device.device_name

        # Get AV controller directly from device for screenshot capture
        self.av_controller = device._get_controller('av')
        if not self.av_controller:
            print(f"[@verification_executor] Warning: No AV controller found for device {self.device_id}")
        
        # Get verification controllers directly by type
        verification_controllers = device.get_controllers('verification')
        self.video_controller = None
        self.image_controller = None
        self.text_controller = None
        self.audio_controller = None
        self.adb_controller = None
        self.appium_controller = None
        self.web_controller = None
        
        for ctrl in verification_controllers:
            class_name = ctrl.__class__.__name__.lower()
            if 'video' in class_name:
                self.video_controller = ctrl
            elif 'image' in class_name:
                self.image_controller = ctrl
            elif 'text' in class_name:
                self.text_controller = ctrl
            elif 'audio' in class_name:
                self.audio_controller = ctrl
            elif 'adb' in class_name:
                self.adb_controller = ctrl
            elif 'appium' in class_name:
                self.appium_controller = ctrl
            elif 'playwright' in class_name or 'web' in class_name:
                self.web_controller = ctrl
        
        # Initialize screenshot tracking
        self.verification_screenshots = []
        
        # Initialized for device: {self.device_id}, model: {self.device_model}
    
    def take_screenshot(self) -> Tuple[bool, str, str]:
        """
        Take a screenshot and return base64 data for AI analysis.
        
        Returns:
            tuple: (success, base64_screenshot_data, error_message)
        """
        try:
            # Use remote controller for base64 screenshot data
            remote_controller = self.device._get_controller('remote')
            if not remote_controller:
                return False, "", "No remote controller available"
            
            if not hasattr(remote_controller, 'take_screenshot'):
                return False, "", "Remote controller does not support screenshots"
            
            print(f"[@verification_executor] Taking screenshot using remote controller: {type(remote_controller).__name__}")
            return remote_controller.take_screenshot()
            
        except Exception as e:
            error_msg = f"Screenshot error: {str(e)}"
            print(f"[@verification_executor] {error_msg}")
            return False, "", error_msg
    
    def get_available_context(self, userinterface_name: str = None) -> Dict[str, Any]:
        """
        Get available verification context for AI based on user interface
        
        Args:
            userinterface_name: User interface name for context
            
        Returns:
            Dict with available verifications and their descriptions
        """
        try:
            device_verifications = []
            
            print(f"[@verification_executor] Loading verification context for device: {self.device_id}, model: {self.device_model}")
            
            # Get verification actions from verification controllers - direct device access
            verification_types = ['image', 'text', 'adb', 'appium', 'video', 'audio']
            for v_type in verification_types:
                try:
                    controller = getattr(self, f'{v_type}_controller', None)
                    if controller and hasattr(controller, 'get_available_verifications'):
                        verifications = controller.get_available_verifications()
                        if isinstance(verifications, list):
                            for verification in verifications:
                                device_verifications.append({
                                    'command': verification.get('command', ''),
                                    'action_type': f'verification_{v_type}',
                                    'params': verification.get('params', {}),
                                    'description': verification.get('description', '')
                                })
                except Exception as e:
                    print(f"[@verification_executor] Could not load verification_{v_type} verifications: {e}")
                    continue
            
            print(f"[@verification_executor] Loaded {len(device_verifications)} verifications from controllers")
            
            return {
                'service_type': 'verifications',
                'device_id': self.device_id,
                'device_model': self.device_model,
                'userinterface_name': userinterface_name,
                'available_verifications': device_verifications
            }
            
        except Exception as e:
            print(f"[@verification_executor] Error loading verification context: {e}")
            return {
                'service_type': 'verifications',
                'device_id': self.device_id,
                'device_model': self.device_model,
                'userinterface_name': userinterface_name,
                'available_verifications': []
            }
    
    def execute_verifications(self, 
                            verifications: List[Dict[str, Any]],
                            userinterface_name: str,  # MANDATORY for reference resolution
                            image_source_url: Optional[str] = None,
                            team_id: str = None,
                            context = None,
                            tree_id: Optional[str] = None,
                            node_id: Optional[str] = None,
                            verification_pass_condition: str = 'all'  # NEW: 'all' or 'any'
                           ) -> Dict[str, Any]:
        """
        Execute batch of verifications
        
        Args:
            verifications: List of verification dictionaries
            userinterface_name: User interface name (REQUIRED for reference resolution, e.g., 'horizon_android_tv')
            image_source_url: Optional source image URL for image/text verifications
            team_id: Team ID for database recording
            context: Optional execution context
            tree_id: Navigation tree ID for database recording
            node_id: Navigation node ID for database recording
            verification_pass_condition: Condition for passing ('all' = all must pass, 'any' = any can pass)
            
        Returns:
            Dict with success status, results, and execution statistics
        """
        # Start capturing logs with Tee (print to both terminal and buffer)
        log_buffer = io.StringIO()
        old_stdout = sys.stdout
        old_stderr = sys.stderr
        
        # Tee class to write to multiple streams (terminal + buffer)
        class Tee:
            def __init__(self, *streams):
                self.streams = streams
            def write(self, data):
                for stream in self.streams:
                    stream.write(data)
                    stream.flush()
            def flush(self):
                for stream in self.streams:
                    stream.flush()
        
        try:
            # Redirect stdout/stderr to BOTH terminal and buffer
            sys.stdout = Tee(old_stdout, log_buffer)
            sys.stderr = Tee(old_stderr, log_buffer)
            
            # Reduced logging for cleaner output during KPI scans
            
            # Clear screenshots from previous verification batch (VerificationExecutor is a singleton per device)
            self.verification_screenshots = []
            
            # Validate inputs
            if not verifications:
                return {
                    'success': True,
                    'message': 'No verifications to execute',
                    'results': [],
                    'passed_count': 0,
                    'total_count': 0,
                    'logs': ''
                }
            
            # Filter valid verifications
            valid_verifications = self._filter_valid_verifications(verifications)
            
            if not valid_verifications:
                return {
                    'success': False,
                    'error': 'All verifications were invalid and filtered out',
                    'results': [],
                    'passed_count': 0,
                    'total_count': 0,
                    'logs': log_buffer.getvalue()
                }
            
            results = []
            passed_count = 0
            
            # Execute each verification
            for i, verification in enumerate(valid_verifications):
                verification_type = verification.get('verification_type', 'text')
                
                start_time = time.time()
                result = self._execute_single_verification(verification, userinterface_name, image_source_url, context, team_id)
                execution_time = int((time.time() - start_time) * 1000)
                
                            # Add execution time to result
                result['execution_time_ms'] = execution_time
                results.append(result)
                
                # Count successful verifications
                if result.get('success'):
                    passed_count += 1
                
                # Record execution directly to database
                self._record_verification_to_database(
                    success=result.get('success', False),
                    execution_time_ms=execution_time,
                    message=result.get('message', ''),
                    error_details={'error': result.get('error')} if result.get('error') else None,
                    team_id=team_id,
                    tree_id=tree_id,
                    node_id=node_id
                )
            

            
            # Calculate overall success based on verification_pass_condition
            if verification_pass_condition == 'any':
                # Pass if ANY verification passed (at least one)
                overall_success = passed_count > 0
                print(f"[@lib:verification_executor:execute_verifications] 'any can pass' mode: {passed_count}/{len(valid_verifications)} passed → {'✅ PASS' if overall_success else '❌ FAIL'}")
            else:
                # Default: Pass only if ALL verifications passed
                overall_success = passed_count == len(valid_verifications)
                print(f"[@lib:verification_executor:execute_verifications] 'all must pass' mode: {passed_count}/{len(valid_verifications)} passed → {'✅ PASS' if overall_success else '❌ FAIL'}")
            
            # Extract detailed error information from failed verifications
            error_info = None
            debug_report_path = None  # ✅ Track first debug report path (local path)
            debug_report_url = None  # ✅ Track first debug report URL (HTTP URL)
            if not overall_success and results:
                # Get the first failed verification's message as the primary error
                for result in results:
                    if not result.get('success', False):
                        error_info = result.get('message', 'Verification failed')
                        # ✅ Capture debug report path and URL from first failed verification
                        if result.get('debug_report_path'):
                            debug_report_path = result.get('debug_report_path')
                        if result.get('debug_report_url'):
                            debug_report_url = result.get('debug_report_url')
                        break
            
            # Collect all verification evidence for KPI report
            verification_evidence_list = []
            for result in results:
                if 'verification_evidence' in result:
                    verification_evidence_list.append(result['verification_evidence'])
            
            result = {
                'success': overall_success,
                'total_count': len(valid_verifications),
                'passed_count': passed_count,
                'failed_count': len(valid_verifications) - passed_count,
                'results': results,
                'verification_screenshots': self.verification_screenshots,  # NEW: Include screenshots
                'verification_evidence_list': verification_evidence_list,  # ✅ NEW: All verification evidence for KPI
                'message': f'Batch verification completed: {passed_count}/{len(valid_verifications)} passed',
                'logs': log_buffer.getvalue()  # ✅ NEW: Include captured logs
            }
            
            # Add error information if verifications failed
            if error_info:
                result['error'] = error_info
                # ✅ Include debug report path and URL if available
                if debug_report_path:
                    result['debug_report_path'] = debug_report_path
                if debug_report_url:
                    result['debug_report_url'] = debug_report_url
                
            return result
        finally:
            # Always restore stdout/stderr
            sys.stdout = old_stdout
            sys.stderr = old_stderr
    
    def get_execution_status(self, execution_id: str) -> Dict[str, Any]:
        """
        Get status of async verification execution (called by route polling).
        
        Returns:
            {
                'success': bool,
                'execution_id': str,
                'status': 'running' | 'completed' | 'error',
                'result': dict (if completed),
                'error': str (if error),
                'progress': int,
                'message': str
            }
        """
        import threading
        if not hasattr(self, '_executions'):
            return {
                'success': False,
                'error': f'Execution {execution_id} not found'
            }
        
        if not hasattr(self, '_lock'):
            self._lock = threading.Lock()
        
        with self._lock:
            if execution_id not in self._executions:
                return {
                    'success': False,
                    'error': f'Execution {execution_id} not found'
                }
            
            execution = self._executions[execution_id].copy()
        
        return {
            'success': True,
            'execution_id': execution['execution_id'],
            'status': execution['status'],
            'result': execution.get('result'),
            'error': execution.get('error'),
            'progress': execution.get('progress', 0),
            'message': execution.get('message', ''),
            'elapsed_time_ms': int((time.time() - execution['start_time']) * 1000)
        }
    
    def verify_node(self, node_id: str, userinterface_name: str, team_id: str, tree_id: str = None, image_source_url: Optional[str] = None) -> Dict[str, Any]:
        """
        Execute verifications for a specific node during navigation.
        
        Args:
            node_id: Node ID to verify
            userinterface_name: User interface name (REQUIRED for reference resolution)
            team_id: Team ID
            tree_id: Optional tree ID (will fall back to navigation context if not provided)
            image_source_url: Optional image source URL
            
        Returns:
            Dict with success status and verification results
        """
        print(f"[@lib:verification_executor:verify_node] 🔍 Called with node_id={node_id}, userinterface_name={userinterface_name}, tree_id={tree_id}, team_id={team_id}")
        
        try:
            # Get tree_id from navigation context if not provided (use root tree for unified graph)
            if not tree_id:
                nav_context = self.device.navigation_context
                tree_id = nav_context.get('tree_id')  # Use root tree_id, not current_tree_id
                print(f"[@lib:verification_executor:verify_node] Tree ID from context: {tree_id}")
            
            # ✅ USE ALREADY-LOADED UNIFIED GRAPH (zero database calls, zero cache calls!)
            # NavigationExecutor already loaded the unified graph during navigation
            if not hasattr(self.device, 'navigation_executor') or not self.device.navigation_executor:
                raise ValueError(f"Device {self.device_id} has no NavigationExecutor - cannot verify")
            
            unified_graph = self.device.navigation_executor.unified_graph
            if not unified_graph:
                raise ValueError(f"NavigationExecutor has no unified graph loaded - call load_navigation_tree() first")
            
            print(f"[@lib:verification_executor:verify_node] ✅ Using unified graph from NavigationExecutor ({len(unified_graph.nodes)} nodes)")
            
            # Get node data from unified graph
            if node_id not in unified_graph.nodes:
                print(f"[@lib:verification_executor:verify_node] ⚠️ Node {node_id} not in graph - skipping verification")
                return {'success': False, 'has_verifications': False, 'message': 'Node not found in graph', 'results': []}
            
            node_data = unified_graph.nodes[node_id]
            
            if not node_data:
                print(f"[@lib:verification_executor:verify_node] ⚠️ Node {node_id} has no data - skipping verification")
                return {'success': False, 'has_verifications': False, 'message': 'Node not found in graph', 'results': []}
            
            print(f"[@lib:verification_executor:verify_node] ✅ Node data from graph: {node_data.get('label')} (tree: {node_data.get('tree_name')})")
            
            # Extract verifications from node
            verifications = node_data.get('verifications', [])
            
            if not verifications:
                print(f"[@lib:verification_executor:verify_node] No verifications for node {node_id}")
                return {'success': False, 'has_verifications': False, 'message': 'No verifications defined - cannot verify position', 'results': []}
            
            # Extract verification_pass_condition from node (defaults to 'all')
            verification_pass_condition = node_data.get('verification_pass_condition', 'all')
            
            print(f"[@lib:verification_executor:verify_node] Executing {len(verifications)} verifications for node {node_id} with condition '{verification_pass_condition}'")
            
            # Use actual node's tree_id for database recording (nested tree)
            actual_tree_id = node_data.get('tree_id')
            
            # Execute verifications with proper tree_id and node_id for database recording
            return self.execute_verifications(
                verifications=verifications,
                userinterface_name=userinterface_name,  # MANDATORY parameter
                image_source_url=image_source_url,
                team_id=team_id,
                tree_id=actual_tree_id,  # Use node's actual tree_id for recording
                node_id=node_id,
                verification_pass_condition=verification_pass_condition  # NEW: Pass condition from node
            )
            
        except Exception as e:
            print(f"[@lib:verification_executor:verify_node] Error: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e), 'results': []}
    
    def _filter_valid_verifications(self, verifications: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Filter out invalid verifications"""
        valid_verifications = []
        
        for i, verification in enumerate(verifications):
            verification_type = verification.get('verification_type', 'text')
            
            if not verification.get('command') or verification.get('command', '').strip() == '':
                print(f"[@lib:verification_executor:_filter_valid_verifications] Removing verification {i}: No command specified")
                continue
            
            # Type-specific validation
            if verification_type == 'image':
                if not verification.get('params', {}).get('image_path'):
                    print(f"[@lib:verification_executor:_filter_valid_verifications] Removing verification {i}: No image reference specified")
                    continue
            elif verification_type == 'text':
                if not verification.get('params', {}).get('text'):
                    print(f"[@lib:verification_executor:_filter_valid_verifications] Removing verification {i}: No text specified")
                    continue
            elif verification_type == 'adb':
                if not verification.get('params', {}).get('search_term'):
                    print(f"[@lib:verification_executor:_filter_valid_verifications] Removing verification {i}: No search term specified")
                    continue
            
            valid_verifications.append(verification)
        
        return valid_verifications
    
    def _execute_single_verification(self, verification: Dict[str, Any], userinterface_name: str, image_source_url: Optional[str], context = None, team_id: str = None) -> Dict[str, Any]:
        """Execute a single verification and return standardized result"""
        try:
            verification_type = verification.get('verification_type', 'text')
            
            # Get verification controller directly
            controller = getattr(self, f'{verification_type}_controller', None)
            if not controller:
                return {
                    'success': False,
                    'error': f'No {verification_type} verification controller found for device {self.device_id}',
                    'verification_type': verification_type,
                    'resultType': 'FAIL'
                }
            
            # Execute verification directly using controller
            verification_config = {
                'command': verification.get('command'),
                'params': verification.get('params', {}),
                'verification_type': verification_type,
                'team_id': team_id,  # Pass team_id for database operations
                'userinterface_name': userinterface_name  # Use mandatory parameter (no fallback)
            }
            
            # Add source_image_path to config if provided (for offline/post-processing)
            if image_source_url:
                import os
                from shared.src.lib.utils.build_url_utils import convertHostUrlToLocalPath
                
                # Helper function to check if path needs conversion
                def needs_conversion(path):
                    """Check if path is a URL that needs conversion (vs already a local path)"""
                    # Only convert HTTP URLs - all local file paths (including /tmp/) should pass through
                    return path.startswith(('http://', 'https://'))
                
                # Handle comma-separated paths (for multiple images in subtitle/audio detection)
                if isinstance(image_source_url, str) and ',' in image_source_url:
                    # Split comma-separated paths and convert each if needed
                    paths = [path.strip() for path in image_source_url.split(',')]
                    processed_paths = []
                    for path in paths:
                        if needs_conversion(path):
                            try:
                                converted_path = convertHostUrlToLocalPath(path)
                                processed_paths.append(converted_path)
                                print(f"[@lib:verification_executor:_execute_single_verification] Converted URL: {path} -> {converted_path}")
                            except Exception as e:
                                print(f"[@lib:verification_executor:_execute_single_verification] Warning: Failed to convert path {path}: {e}")
                        else:
                            # Already a local path, use as-is
                            processed_paths.append(path)
                    
                    if processed_paths:
                        source_image_path = ','.join(processed_paths)
                        verification_config['source_image_path'] = source_image_path
                        print(f"[@lib:verification_executor:_execute_single_verification] Using {len(processed_paths)} image path(s)")
                    else:
                        print(f"[@lib:verification_executor:_execute_single_verification] Warning: No valid paths available")
                else:
                    # Single path
                    if needs_conversion(image_source_url):
                        source_image_path = convertHostUrlToLocalPath(image_source_url)
                        verification_config['source_image_path'] = source_image_path
                    else:
                        # Already a local path, use as-is
                        verification_config['source_image_path'] = image_source_url
            
            # Set context on controller so helpers can access it (for motion image collection)
            if context:
                controller._current_context = context
            
            # Direct controller execution
            verification_result = controller.execute_verification(verification_config)
            
            # Build URLs from file paths if verification generated images
            # Frontend will process these paths using buildVerificationResultUrl from buildUrlUtils.ts
            details = verification_result.get('details', {})
            if details.get('source_image_path'):
                verification_result['sourceUrl'] = details['source_image_path']  # Local path - frontend converts to URL
            if details.get('reference_image_url'):
                verification_result['referenceUrl'] = details['reference_image_url']  # Already R2 URL
            if details.get('result_overlay_path'):
                verification_result['overlayUrl'] = details['result_overlay_path']  # Local path - frontend converts to URL
            
            # Capture screenshot (no upload)
            from shared.src.lib.utils.device_utils import capture_screenshot
            screenshot_path = capture_screenshot(self.device, context) or ""
            
            # Add screenshot to collection for report
            if screenshot_path:
                self.verification_screenshots.append(screenshot_path)
            
            # Build verification evidence for KPI report (NEW)
            verification_evidence = {
                'type': verification_type,
                'command': verification.get('command'),
                'success': verification_result.get('success', False),
                'params': verification.get('params', {}),
            }
            
            # Add type-specific evidence
            if verification_type == 'image':
                verification_evidence.update({
                    'reference_image_path': details.get('reference_image_path'),  # Local cropped reference
                    'source_image_path': details.get('source_image_path'),  # Local cropped source
                    'threshold': verification_result.get('threshold', 0.8),
                    'matching_score': verification_result.get('matching_result', 0.0),
                    'search_area': verification.get('params', {}).get('area'),
                    'image_filter': verification_result.get('imageFilter', 'none'),
                })
            elif verification_type == 'text':
                verification_evidence.update({
                    'source_image_path': details.get('source_image_path'),
                    'searched_text': verification_result.get('searchedText', ''),
                    'extracted_text': verification_result.get('extractedText', ''),
                    'language': verification_result.get('detected_language', 'unknown'),
                    'confidence': verification_result.get('language_confidence', 0),
                })
            
            flattened_result = {
                'success': verification_result.get('success', False),
                'message': verification_result.get('message'),
                'error': verification_result.get('error'),
                'threshold': verification_result.get('threshold') or verification_result.get('confidence') or verification_result.get('userThreshold', 0.8),
                'matching_result': verification_result.get('matching_result'),  # ✅ Add for report generator
                'resultType': 'PASS' if verification_result.get('success', False) else 'FAIL',
                'sourceImageUrl': verification_result.get('sourceUrl'),
                'referenceImageUrl': verification_result.get('referenceUrl'),
                'resultOverlayUrl': verification_result.get('overlayUrl'),
                'extractedText': verification_result.get('extractedText', ''),
                'searchedText': verification_result.get('searchedText', ''),
                'imageFilter': verification_result.get('imageFilter', 'none'),
                'detectedLanguage': verification_result.get('detected_language'),
                'languageConfidence': verification_result.get('language_confidence'),
                # ADB-specific fields
                'search_term': verification_result.get('search_term'),
                'wait_time': verification_result.get('wait_time'),
                'total_matches': verification_result.get('total_matches'),
                'matches': verification_result.get('matches'),
                # Appium-specific fields
                'platform': verification_result.get('platform'),
                # Audio/Video-specific fields  
                'motion_threshold': verification_result.get('motion_threshold'),
                'duration': verification_result.get('duration'),
                'frequency': verification_result.get('frequency'),
                'audio_level': verification_result.get('audio_level'),
                # General fields
                'verification_type': verification_result.get('verification_type', verification_type),
                'execution_time_ms': verification_result.get('execution_time_ms'),
                'details': verification_result.get('details', {}),
                'screenshot_path': screenshot_path,  # Always present
                'verification_evidence': verification_evidence  # ✅ NEW: Evidence data for KPI report
            }
            
            # Clean up context reference to avoid memory leaks
            if hasattr(controller, '_current_context'):
                delattr(controller, '_current_context')
            
            # Generate debug report for failures (only for image/text verifications with images)
            if not flattened_result.get('success') and verification_type in ['image', 'text']:
                print(f"[@lib:verification_executor] " + "=" * 80)
                print(f"[@lib:verification_executor] 🔍 VERIFICATION FAILURE - GENERATING DEBUG REPORT")
                print(f"[@lib:verification_executor] " + "=" * 80)
                try:
                    from shared.src.lib.utils.verification_report_generator import generate_verification_failure_report
                    from shared.src.lib.utils.storage_path_utils import get_capture_folder
                    from backend_host.src.lib.utils.host_utils import get_host_instance
                    
                    # ✅ Get device folder from AV controller (same as image.py does)
                    device_folder = get_capture_folder(self.av_controller.video_capture_path)
                    print(f"[@lib:verification_executor] Device folder from AV controller: {device_folder}")
                    
                    # ✅ Get host info for URL building
                    host = get_host_instance()
                    host_info = {
                        'host_name': host.host_name,
                        'host_url': host.host_url  # Already set during registration with proper env var
                    }
                    print(f"[@lib:verification_executor] Host info for report: {host_info}")
                    
                    # ✅ Get source_image_path from RESULT (controller sets it), not from config (input)
                    # Controllers auto-capture screenshots if not provided, so path is in result.details
                    source_path = None
                    details = flattened_result.get('details', {})
                    
                    # DEBUG: Show what's in details
                    print(f"[@lib:verification_executor] Details dict has {len(details)} keys: {list(details.keys())}")
                    
                    # Try multiple locations where source path might be
                    if 'source_image_path' in details:
                        source_path = details['source_image_path']
                        print(f"[@lib:verification_executor] ✅ Found source_image_path in details: {source_path}")
                    elif 'source_image_path' in verification_config:
                        source_path = verification_config['source_image_path']
                        print(f"[@lib:verification_executor] ✅ Found source_image_path in config: {source_path}")
                    else:
                        print(f"[@lib:verification_executor] ❌ source_image_path not found!")
                        print(f"[@lib:verification_executor] Details content: {details}")
                    
                    if source_path and device_folder:
                        # ✅ Add source_image_path to verification_config for report generator
                        verification_config_with_path = {**verification_config, 'source_image_path': source_path}
                        
                        # CRITICAL: Pass flattened_result (has correct field mapping) not verification_result (raw)
                        report_path = generate_verification_failure_report(
                            verification_config=verification_config_with_path,
                            verification_result=flattened_result,  # ✅ Use flattened_result with correct field mapping
                            device_folder=device_folder,
                            host_info=host_info  # ✅ Pass host_info for URL building
                        )
                        if report_path:
                            print(f"[@lib:verification_executor] " + "-" * 80)
                            print(f"[@lib:verification_executor] 🔍 DEBUG REPORT (local): {report_path}")
                            
                            # ✅ Convert local path to URL for frontend
                            from shared.src.lib.utils.build_url_utils import buildHostImageUrl
                            report_url = buildHostImageUrl(host_info, report_path)
                            
                            print(f"[@lib:verification_executor] 🔍 DEBUG REPORT (URL): {report_url}")
                            print(f"[@lib:verification_executor] " + "-" * 80)
                            
                            # ✅ ADD REPORT URL TO RESULT for frontend to display as link
                            flattened_result['debug_report_url'] = report_url
                            flattened_result['debug_report_path'] = report_path  # Keep local path for backend reference
                        else:
                            print(f"[@lib:verification_executor] ⚠️ Report generation returned None")
                    else:
                        if not source_path:
                            print(f"[@lib:verification_executor] ⚠️ Source path not found - cannot generate report")
                        if not device_folder:
                            print(f"[@lib:verification_executor] ⚠️ Device folder is None - cannot generate report")
                        
                    print(f"[@lib:verification_executor] " + "=" * 80)
                except Exception as report_error:
                    # Don't let report generation break verification execution
                    print(f"[@lib:verification_executor] ❌ Failed to generate debug report: {report_error}")
                    import traceback
                    traceback.print_exc()
                    print(f"[@lib:verification_executor] " + "=" * 80)
            
            return flattened_result
            
        except Exception as e:
            # EXCEPTION HANDLER: This should only trigger for unexpected crashes, not normal verification failures
            # Normal failures should return {'success': False} from the controller, not throw exceptions
            print(f"[@lib:verification_executor:_execute_single_verification] ⚠️ UNEXPECTED EXCEPTION in verification:")
            print(f"[@lib:verification_executor:_execute_single_verification]   Verification type: {verification.get('verification_type', 'unknown')}")
            print(f"[@lib:verification_executor:_execute_single_verification]   Command: {verification.get('command', 'unknown')}")
            print(f"[@lib:verification_executor:_execute_single_verification]   Exception: {str(e)}")
            print(f"[@lib:verification_executor:_execute_single_verification]   Full traceback:")
            import traceback
            traceback.print_exc()
            
            # Capture error screenshot but DON'T add to validation context (use context=None)
            # This screenshot is for debugging the exception, not for the validation report
            from shared.src.lib.utils.device_utils import capture_screenshot
            screenshot_path = capture_screenshot(self.device, context=None) or ""
            
            # Add to internal collection for debugging, but it won't appear in validation report
            if screenshot_path:
                self.verification_screenshots.append(screenshot_path)
                print(f"[@lib:verification_executor:_execute_single_verification] 📸 Exception screenshot captured (for debugging, not validation report): {screenshot_path}")
            
            return {
                'success': False,
                'message': f"Verification execution failed with exception",
                'error': str(e),
                'verification_type': verification.get('verification_type', 'unknown'),
                'resultType': 'FAIL',
                'screenshot_path': screenshot_path  # Always present
            }
    
    def _record_verification_to_database(self, success: bool, execution_time_ms: int, message: str, error_details: Optional[Dict] = None, team_id: str = None, tree_id: Optional[str] = None, node_id: Optional[str] = None):
        """Record single verification directly to database"""
        try:
            # Get navigation context from device
            nav_context = self.device.navigation_context
            
            # Use provided tree_id/node_id if available, otherwise fall back to navigation context
            if not tree_id:  # Handles None and empty string
                tree_id = nav_context.get('current_tree_id')
            if not node_id:  # Handles None and empty string
                node_id = nav_context.get('current_node_id')
            
            # Only record if we have valid tree_id, node_id, and team_id (not None or empty string)
            if not tree_id or not node_id:
                print(f"[@lib:verification_executor:_record_verification_to_database] Skipping database recording - missing navigation context (tree_id: {tree_id}, node_id: {node_id})")
                return
            
            if team_id is None:
                print(f"[@lib:verification_executor:_record_verification_to_database] Skipping database recording - missing team_id")
                return
            
            # Get script context from device navigation_context - single source of truth
            script_result_id = nav_context.get('script_id')
            script_context = nav_context.get('script_context', 'direct')
            
            record_node_execution(
                team_id=team_id,
                tree_id=tree_id,
                node_id=node_id,
                host_name=self.host_name,
                device_model=self.device_model,
                device_name=self.device_name,
                success=success,
                execution_time_ms=execution_time_ms,
                message=message,
                error_details=error_details,
                script_result_id=script_result_id,
                script_context=script_context
            )
            
        except Exception as e:
            print(f"[@lib:verification_executor:_record_verification_to_database] Database recording error: {e}") 