#!/usr/bin/env python3
"""
HOT/COLD STORAGE ARCHIVER - Progressive MP4 Optimization
=========================================================

Responsibilities:
1. Progressive MP4 merging: 6s TS → 6s MP4 → 1min MP4 → 10min MP4
2. Archive metadata to hour folders
3. Clean old files to maintain RAM limits
4. 98% disk write reduction through progressive grouping
"""

import os
import sys
import time
import shutil
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Tuple, Optional

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, project_root)

from shared.src.lib.utils.storage_path_utils import get_capture_base_directories, is_ram_mode
from shared.src.lib.utils.video_utils import merge_progressive_batch

# Configure logging (systemd handles file output)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [HOT_COLD_ARCHIVER] %(levelname)s: %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# Configuration
# Note: ACTIVE_CAPTURES_FILE managed by run_ffmpeg_and_rename_local.sh (not used here)
ACTIVE_CAPTURES_FILE = '/tmp/active_captures.conf'
RAM_RUN_INTERVAL = 60   # 1min for RAM mode (faster cleanup for video content)
SD_RUN_INTERVAL = 60    # 1min (same for consistency)

# Hot storage limits - ALIGNED WITH 60s RUN INTERVAL
# Captures pushed to cloud immediately, keep 60s buffer (same as archiver interval)
# Thumbnails generated by FFmpeg at same rate as captures (5fps v4l2, 2fps VNC)
# Segments archived to hour folders for 24h rolling access
# Metadata and transcripts kept in HOT only (light: 1.4MB total, no disk writes)
#
# RAM Usage (HIGH QUALITY CAPTURES - Video content worst case):
# - Segments: 150 × 38KB = 6MB (2.5min buffer, archive every 1min)
# - Captures: 300 × 245KB = 74MB (60s buffer at 5fps, HD captures at 1280x720 for best detection)
# - Thumbnails: 100 × 28KB = 3MB (for freeze detection comparisons)
# - Metadata: 100 × 2KB = 0.2MB (JSON files, KEPT IN HOT - light data)
# - Transcripts: 24 × 50KB = 1.2MB (hourly transcript files, KEPT IN HOT - light data)
# Total: ~84.4MB per device ✅ (42% of 200MB budget, high quality preserved)
#
HOT_LIMITS = {
    'segments': 150,      # 2.5min buffer → archive to hour folders
    'captures': 300,      # 60s buffer at 5fps (same as archiver interval)
    'thumbnails': 100,    # For freeze detection (last 3 frames comparison)
    # Metadata and transcripts kept in HOT only (light data: 1.4MB total, no archiving)
}

# REMOVED: RETENTION_HOURS config
# 
# WHY: Natural 24h rolling buffer through time-based sequential filenames
# All files naturally overwrite after 24h - no retention configuration needed!
# 
# How it works:
# - Files get time-based names based on seconds since midnight
# - After 24h, same time → same filename → automatic overwrite
# - Result: All hour folders maintain 24h of data automatically

# File patterns for archiving (only heavy data)
# Metadata and transcripts NOT archived - kept in HOT only (light: 1.4MB total)
FILE_PATTERNS = {
    'segments': 'segment_*.ts',
    'captures': 'capture_*[0-9].jpg',  # Full captures only (rotated, not archived)
}


# Use centralized function from archive_utils.py
# No local implementation needed - single source of truth!


def get_file_hour(filepath: str) -> int:
    """Get hour (0-23) from file modification time"""
    try:
        mtime = os.path.getmtime(filepath)
        return datetime.fromtimestamp(mtime).hour
    except Exception as e:
        logger.error(f"Error getting file hour for {filepath}: {e}")
        return datetime.now().hour


def calculate_time_based_name(filepath: str, file_type: str, fps: int = 5) -> str:
    """
    Calculate time-based sequential filename for 24h rolling buffer
    
    Uses file mtime to calculate position in 24h cycle:
    - Segments (1s): 0-86399 (24h × 3600s)
    - Images (5fps): 0-431999 (86400s × 5fps)
    - Images (2fps): 0-172799 (86400s × 2fps)
    - Transcripts: Keep original name (transcript_hourX.json)
    
    Args:
        filepath: Original file path
        file_type: 'segments', 'captures', 'metadata', 'transcripts'
        fps: Frames per second (for images only)
    
    Returns:
        New filename with time-based sequential number or original name for transcripts
    """
    try:
        # Special case: transcripts keep their original name (transcript_hour13.json)
        if file_type == 'transcripts':
            return os.path.basename(filepath)
        
        mtime = os.path.getmtime(filepath)
        dt = datetime.fromtimestamp(mtime)
        
        # Calculate seconds since midnight
        seconds_today = (dt.hour * 3600) + (dt.minute * 60) + dt.second
        
        if file_type == 'segments':
            # 1 segment per second: 0-86399
            sequence_num = seconds_today
            new_name = f"segment_{sequence_num:06d}.ts"
        elif file_type in ['captures', 'metadata']:
            # Images at FPS rate
            sequence_num = seconds_today * fps
            
            # Get original filename to extract extension and type
            original_name = os.path.basename(filepath)
            
            if file_type == 'metadata':
                new_name = f"capture_{sequence_num:06d}.json"
            else:  # captures
                new_name = f"capture_{sequence_num:06d}.jpg"
        else:
            # Unknown type, keep original name
            return os.path.basename(filepath)
        
        return new_name
        
    except Exception as e:
        logger.error(f"Error calculating time-based name for {filepath}: {e}")
        return os.path.basename(filepath)


def archive_hot_files(capture_dir: str, file_type: str) -> int:
    """
    Archive hot files to hour folders when exceeding limit
    
    **RAM MODE:** Reads from /hot/captures/, archives to /captures/X/
    **SD MODE:** Reads from /captures/, archives to /captures/X/
    
    GUARANTEES:
    - Always keeps the NEWEST hot_limit files in hot storage
    - Never archives files less than 60 seconds old (safety buffer)
    - Verifies correct file count after archiving
    
    Returns: Number of files archived
    """
    ram_mode = is_ram_mode(capture_dir)
    
    # Determine hot and cold paths based on mode
    if ram_mode:
        # RAM mode: hot storage in /hot/ subdirectory
        hot_dir = os.path.join(capture_dir, 'hot', file_type)
        cold_dir = os.path.join(capture_dir, file_type)
    else:
        # SD mode: hot storage in root, cold in hour subfolders
        hot_dir = os.path.join(capture_dir, file_type)
        cold_dir = hot_dir  # Same directory, hour folders are subdirs
    
    if not os.path.isdir(hot_dir):
        return 0
    
    pattern = FILE_PATTERNS[file_type]
    hot_limit = HOT_LIMITS[file_type]
    
    # Get files in hot storage (root only, not subdirs)
    try:
        files = []
        current_time = time.time()
        
        for item in Path(hot_dir).glob(pattern):
            if item.is_file() and item.parent == Path(hot_dir):
                files.append(item)
        
        file_count = len(files)
        
        if file_count < hot_limit:
            logger.debug(f"{file_type}: {file_count} files (within limit {hot_limit})")
            return 0
        
        # Calculate how many to archive
        to_archive = file_count - hot_limit
        
        # Sort by modification time (oldest first) - CRITICAL for keeping newest files
        files.sort(key=lambda f: f.stat().st_mtime)
        
        # Safety check: Never archive files less than 30 seconds old
        # This protects restart video operations from race conditions
        MIN_AGE_SECONDS = 30
        files_to_archive = []
        files_too_recent = []
        
        for filepath in files[:to_archive]:
            file_age = current_time - filepath.stat().st_mtime
            if file_age >= MIN_AGE_SECONDS:
                files_to_archive.append(filepath)
            else:
                files_too_recent.append(filepath)
        
        if files_too_recent:
            logger.warning(f"{file_type}: {len(files_too_recent)} files too recent to archive (< {MIN_AGE_SECONDS}s old) - keeping in hot storage for safety")
        
        if not files_to_archive:
            logger.info(f"{file_type}: {file_count} files, but all recent files (< {MIN_AGE_SECONDS}s old) - skipping archival for safety")
            return 0
        
        # Log what we're about to do
        oldest_file_age = current_time - files_to_archive[0].stat().st_mtime
        newest_kept_age = current_time - files[-1].stat().st_mtime
        logger.info(f"{file_type}: Archiving {len(files_to_archive)} old files ({file_count} → {hot_limit + len(files_too_recent)}, oldest={oldest_file_age:.1f}s, newest_kept={newest_kept_age:.1f}s)")
        
        archived_count = 0
        for filepath in files_to_archive:
            try:
                # Get hour from file mtime
                file_hour = get_file_hour(str(filepath))
                hour_folder = os.path.join(cold_dir, str(file_hour))
                
                # Ensure hour folder exists
                os.makedirs(hour_folder, exist_ok=True)
                
                # Calculate time-based sequential name for 24h rolling buffer
                # FPS detection: segments=1fps, captures/metadata=5fps default
                fps = 5 if file_type in ['captures', 'metadata'] else 1
                new_filename = calculate_time_based_name(str(filepath), file_type, fps)
                
                # Move file to hour folder with time-based name (RAM → SD or SD root → SD hour)
                dest_path = os.path.join(hour_folder, new_filename)
                
                # If file exists (24h rollover), overwrite it (natural rolling buffer behavior)
                if os.path.exists(dest_path):
                    logger.debug(f"Overwriting existing {new_filename} (24h rollover)")
                    os.remove(dest_path)
                
                shutil.move(str(filepath), dest_path)
                
                archived_count += 1
                mode_label = "RAM→SD" if ram_mode else "hot→cold"
                logger.debug(f"Archived {filepath.name} → {file_type}/{file_hour}/{new_filename} ({mode_label})")
                
            except Exception as e:
                logger.error(f"Error archiving {filepath}: {e}")
        
        # Verify: Count remaining files in hot storage
        remaining_files = [f for f in Path(hot_dir).glob(pattern) if f.is_file() and f.parent == Path(hot_dir)]
        remaining_count = len(remaining_files)
        
        if remaining_count > hot_limit + 10:  # Allow 10 file buffer for race conditions
            logger.warning(f"{file_type}: After archiving, hot storage still has {remaining_count} files (expected ~{hot_limit})")
        else:
            logger.info(f"{file_type}: ✓ Verified hot storage has {remaining_count} files (target: {hot_limit})")
        
        return archived_count
        
    except Exception as e:
        logger.error(f"Error archiving {file_type} files: {e}")
        return 0




def rotate_hot_captures(capture_dir: str) -> int:
    """
    Rotate hot captures - keep only newest 300 files, DELETE old ones.
    
    Captures don't go to cold storage (pushed to cloud), so we just delete old files.
    This keeps RAM usage under control (60s buffer = 74MB worst case for video content).
    
    Returns: Number of files deleted
    """
    ram_mode = is_ram_mode(capture_dir)
    
    # Determine hot path based on mode
    if ram_mode:
        hot_dir = os.path.join(capture_dir, 'hot', 'captures')
    else:
        hot_dir = os.path.join(capture_dir, 'captures')
    
    if not os.path.isdir(hot_dir):
        return 0
    
    hot_limit = HOT_LIMITS['captures']
    
    try:
        # Get all capture files in hot storage
        files = []
        for item in Path(hot_dir).glob('capture_*[0-9].jpg'):
            if item.is_file() and item.parent == Path(hot_dir):
                files.append(item)
        
        file_count = len(files)
        
        if file_count < hot_limit:
            logger.debug(f"captures: {file_count} files (within limit {hot_limit})")
            return 0
        
        # Calculate how many to delete
        to_delete = file_count - hot_limit
        
        # Sort by modification time (oldest first) - CRITICAL for keeping newest files
        files.sort(key=lambda f: f.stat().st_mtime)
        
        # Delete oldest files
        deleted_count = 0
        for filepath in files[:to_delete]:
            try:
                os.remove(str(filepath))
                deleted_count += 1
            except Exception as e:
                logger.error(f"Error deleting {filepath}: {e}")
        
        logger.info(f"captures: Deleted {deleted_count} old files ({file_count} → {file_count - deleted_count}, target: {hot_limit})")
        
        return deleted_count
        
    except Exception as e:
        logger.error(f"Error rotating captures: {e}")
        return 0


def clean_old_thumbnails(capture_dir: str) -> int:
    """
    Clean old thumbnails from /hot/thumbnails/ directory - keep only newest 100 files.
    
    Thumbnails are generated by FFmpeg at same rate as captures (5fps v4l2, 2fps VNC).
    We keep a small buffer (100 files = ~3MB) for freeze detection comparisons.
    Old thumbnails are deleted to save RAM.
    
    Returns: Number of files deleted
    """
    ram_mode = is_ram_mode(capture_dir)
    
    # Thumbnails are in separate directory for better organization
    if ram_mode:
        hot_dir = os.path.join(capture_dir, 'hot', 'thumbnails')
    else:
        hot_dir = os.path.join(capture_dir, 'thumbnails')
    
    if not os.path.isdir(hot_dir):
        return 0
    
    # Keep only 100 thumbnails (FFmpeg-generated, for freeze detection comparisons)
    thumbnail_limit = HOT_LIMITS.get('thumbnails', 100)
    
    try:
        # Get all thumbnail files (*_thumbnail.jpg)
        files = []
        for item in Path(hot_dir).glob('capture_*_thumbnail.jpg'):
            if item.is_file() and item.parent == Path(hot_dir):
                files.append(item)
        
        file_count = len(files)
        
        if file_count < thumbnail_limit:
            if file_count > 0:
                logger.debug(f"thumbnails: {file_count} files (within limit {thumbnail_limit})")
            return 0
        
        # Calculate how many to delete
        to_delete = file_count - thumbnail_limit
        
        # Sort by modification time (oldest first)
        files.sort(key=lambda f: f.stat().st_mtime)
        
        # Delete oldest thumbnails
        deleted_count = 0
        for filepath in files[:to_delete]:
            try:
                os.remove(str(filepath))
                deleted_count += 1
            except Exception as e:
                logger.error(f"Error deleting thumbnail {filepath}: {e}")
        
        logger.info(f"thumbnails: Deleted {deleted_count} old files ({file_count} → {file_count - deleted_count}, target: {thumbnail_limit})")
        
        return deleted_count
        
    except Exception as e:
        logger.error(f"Error cleaning thumbnails: {e}")
        return 0


def process_capture_directory(capture_dir: str):
    """
    Process single capture directory:
    1. Rotate captures (delete old, keep newest 300 = 60s buffer)
    2. Clean old thumbnails (keep newest 100 for freeze detection)
    3. Progressive MP4 merging (6s → 1min → 10min)
    
    Note: Metadata and transcripts kept in HOT only (light: 1.4MB, no archiving)
    """
    logger.info(f"Processing {capture_dir}")
    
    start_time = time.time()
    
    deleted_captures = rotate_hot_captures(capture_dir)
    deleted_thumbnails = clean_old_thumbnails(capture_dir)
    
    ram_mode = is_ram_mode(capture_dir)
    hot_segments = os.path.join(capture_dir, 'hot', 'segments') if ram_mode else os.path.join(capture_dir, 'segments')
    temp_dir = os.path.join(capture_dir, 'segments', 'temp')
    os.makedirs(temp_dir, exist_ok=True)
    
    mp4_6s = merge_progressive_batch(hot_segments, 'segment_*.ts', os.path.join(temp_dir, f'6s_{int(time.time())}.mp4'), 6, True, 10)
    if mp4_6s:
        logger.info("Created 6s MP4")
    
    mp4_1min = merge_progressive_batch(temp_dir, '6s_*.mp4', os.path.join(temp_dir, f'1min_{int(time.time())}.mp4'), 10, True, 15)
    if mp4_1min:
        logger.info("Created 1min MP4")
    
    hour = datetime.now().hour
    hour_dir = os.path.join(capture_dir, 'segments', str(hour))
    os.makedirs(hour_dir, exist_ok=True)
    chunk_index = len(list(Path(hour_dir).glob('chunk_10min_*.mp4')))
    mp4_path = os.path.join(hour_dir, f'chunk_10min_{chunk_index}.mp4')
    mp4_10min = merge_progressive_batch(temp_dir, '1min_*.mp4', mp4_path, 10, True, 20)
    if mp4_10min:
        logger.info(f"Created 10min chunk: {hour}/chunk_10min_{chunk_index}.mp4")
        
        # Extract audio from 10-min MP4 for separate audio archive (aligned with transcripts)
        audio_path = os.path.join(hour_dir, f'chunk_10min_{chunk_index}_audio.mp3')
        try:
            import subprocess
            # Extract audio using FFmpeg: MP4 → MP3
            subprocess.run(
                ['ffmpeg', '-i', mp4_path, '-vn', '-acodec', 'libmp3lame', '-q:a', '4', audio_path, '-y'],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                check=True,
                timeout=30
            )
            logger.info(f"Extracted audio: {hour}/chunk_10min_{chunk_index}_audio.mp3")
        except Exception as e:
            logger.warning(f"Failed to extract audio from chunk {chunk_index}: {e}")
    
    elapsed = time.time() - start_time
    mp4_status = [s for s, result in [("6s", mp4_6s), ("1min", mp4_1min), ("10min", mp4_10min)] if result]
    mp4_info = f", MP4: {'+'.join(mp4_status)}" if mp4_status else ""
    
    logger.info(f"✓ Completed in {elapsed:.2f}s (del: {deleted_captures} cap, {deleted_thumbnails} thumb{mp4_info})")


def main_loop():
    """
    Main service loop - Progressive MP4 merging
    Processes every 1min: 6s→1min→10min MP4 grouping
    """
    logger.info("=" * 60)
    logger.info("HOT/COLD ARCHIVER - PROGRESSIVE MP4 OPTIMIZATION")
    logger.info("=" * 60)
    
    # Detect mode from first capture directory
    capture_dirs = get_capture_base_directories()
    ram_mode = any(is_ram_mode(d) for d in capture_dirs if os.path.exists(d))
    run_interval = RAM_RUN_INTERVAL if ram_mode else SD_RUN_INTERVAL
    
    mode_name = "RAM MODE" if ram_mode else "SD MODE"
    logger.info(f"Mode: {mode_name}")
    logger.info(f"Run interval: {run_interval}s")
    logger.info(f"Hot limits: {HOT_LIMITS}")
    logger.info("Strategy: Progressive MP4 (6s→1min→10min), light data kept in HOT")
    logger.info("=" * 60)
    
    while True:
        try:
            cycle_start = time.time()
            
            logger.info("")
            logger.info("=" * 60)
            logger.info(f"Starting archival cycle at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"Current hour: {datetime.now().hour}")
            logger.info("=" * 60)
            
            # Get active capture directories
            capture_dirs = get_capture_base_directories()
            logger.info(f"Processing {len(capture_dirs)} capture directories")
            
            # Process each directory
            for capture_dir in capture_dirs:
                try:
                    process_capture_directory(capture_dir)
                except Exception as e:
                    logger.error(f"Error processing {capture_dir}: {e}", exc_info=True)
            
            cycle_elapsed = time.time() - cycle_start
            logger.info("=" * 60)
            logger.info(f"Cycle completed in {cycle_elapsed:.2f}s")
            logger.info(f"Next run in {run_interval}s")
            logger.info("=" * 60)
            
            # Sleep until next cycle
            time.sleep(run_interval)
            
        except KeyboardInterrupt:
            logger.info("Received interrupt signal, shutting down...")
            break
        except Exception as e:
            logger.error(f"Error in main loop: {e}", exc_info=True)
            time.sleep(run_interval)


if __name__ == '__main__':
    try:
        main_loop()
    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        sys.exit(1)

