# AI Test Case Generation - Complete Implementation Plan

## üéØ Overview

Create an AI-powered test case generation system where:

1. **Main TestCase page shows existing test cases** in table format
2. **"Create Test Case" button opens dialog** with AI generator
3. **Two-step process**: Analysis ‚Üí Generation
4. **AI analyzes ALL userinterfaces** for compatibility
5. **Individual test cases stored** with creator distinction (AI/Manual)
6. **Clean modern implementation** - no fallbacks or legacy code

---

## üóÑÔ∏è Database Schema

### Enhanced `test_cases` Table
```sql
-- Add new columns to existing test_cases table
ALTER TABLE test_cases ADD COLUMN IF NOT EXISTS creator VARCHAR(20) DEFAULT 'manual';
ALTER TABLE test_cases ADD COLUMN IF NOT EXISTS original_prompt TEXT;
ALTER TABLE test_cases ADD COLUMN IF NOT EXISTS ai_analysis JSONB;
ALTER TABLE test_cases ADD COLUMN IF NOT EXISTS compatible_devices TEXT[];
ALTER TABLE test_cases ADD COLUMN IF NOT EXISTS compatible_userinterfaces TEXT[];
ALTER TABLE test_cases ADD COLUMN IF NOT EXISTS device_adaptations JSONB;
```

### New `ai_analysis_cache` Table
```sql
CREATE TABLE ai_analysis_cache (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    prompt TEXT NOT NULL,
    analysis_result JSONB NOT NULL,
    compatibility_matrix JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    expires_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() + INTERVAL '1 hour'
);
```

---

## üé® Frontend Implementation

### 1. TestCase Page (`frontend/src/pages/TestCaseEditor.tsx`)

**Structure**: Main table + Enhanced create dialog

```tsx
function TestCaseEditor() {
  const [createDialogOpen, setCreateDialogOpen] = useState(false);
  const [testCases, setTestCases] = useState([]);

  return (
    <div className="testcase-page">
      {/* KEEP EXISTING: Header + Create Button */}
      <PageHeader 
        title="Test Cases"
        actions={
          <Button 
            onClick={() => setCreateDialogOpen(true)}
            variant="contained"
          >
            Create Test Case
          </Button>
        }
      />

      {/* KEEP EXISTING: Table with NEW Creator Column */}
      <TestCaseTable 
        testCases={testCases}
        columns={[
          'name', 
          'description', 
          'creator', // NEW: Shows AI vs Manual badge
          'compatible_devices', 
          'last_execution',
          'actions'
        ]}
        onEdit={handleEdit}
        onDelete={handleDelete}
        onExecute={handleExecute}
      />

      {/* NEW: Enhanced Create Dialog */}
      <Dialog 
        open={createDialogOpen} 
        onClose={() => setCreateDialogOpen(false)}
        maxWidth="md"
        fullWidth
      >
        <DialogTitle>Create New Test Case</DialogTitle>
        <DialogContent>
          <AITestCaseGenerator 
            onTestCasesCreated={(newTestCases) => {
              setTestCases([...testCases, ...newTestCases]);
              setCreateDialogOpen(false);
            }}
            onCancel={() => setCreateDialogOpen(false)}
          />
        </DialogContent>
      </Dialog>
    </div>
  );
}
```

### 2. AI Generator Component (`frontend/src/components/testcase/AITestCaseGenerator.tsx`)

**Two-Step Process Inside Dialog**:

```tsx
function AITestCaseGenerator({ onTestCasesCreated, onCancel }) {
  const [step, setStep] = useState<'input' | 'analysis' | 'generation'>('input');
  const [prompt, setPrompt] = useState('');
  const [analysis, setAnalysis] = useState(null);
  const [selectedInterfaces, setSelectedInterfaces] = useState([]);
  const { analyzeTestCase, generateTestCases, loading } = useAITestCase();

  return (
    <Box sx={{ minHeight: 450 }}>
      {/* STEP 1: Input */}
      {step === 'input' && (
        <Box>
          <Typography variant="h6" gutterBottom>
            ü§ñ Describe Your Test Case
          </Typography>
          <TextField
            fullWidth
            multiline
            rows={4}
            placeholder="Example: Go to live and check audio"
            value={prompt}
            onChange={(e) => setPrompt(e.target.value)}
            sx={{ mb: 3 }}
          />
          <Box sx={{ display: 'flex', justifyContent: 'space-between' }}>
            <Button onClick={onCancel}>Cancel</Button>
            <Button 
              variant="contained"
              onClick={async () => {
                const result = await analyzeTestCase(prompt);
                setAnalysis(result);
                setStep('analysis');
              }}
              disabled={!prompt.trim() || loading}
            >
              {loading ? 'Analyzing...' : 'Analyze Compatibility'}
            </Button>
          </Box>
        </Box>
      )}

      {/* STEP 2: Analysis Results */}
      {step === 'analysis' && analysis && (
        <Box>
          <Typography variant="h6" gutterBottom>
            üîç Compatibility Analysis
          </Typography>
          
          <Alert severity="info" sx={{ mb: 3 }}>
            <Typography variant="subtitle2">AI Understanding:</Typography>
            <Typography>{analysis.understanding}</Typography>
          </Alert>

          <Typography variant="subtitle1" gutterBottom>
            Compatible User Interfaces:
          </Typography>
          
          <Box sx={{ maxHeight: 250, overflow: 'auto', mb: 3 }}>
            {analysis.compatibility_matrix.compatible_userinterfaces.map(ui => (
              <Card key={ui} sx={{ mb: 1, p: 2 }}>
                <FormControlLabel
                  control={
                    <Checkbox
                      checked={selectedInterfaces.includes(ui)}
                      onChange={(e) => {
                        if (e.target.checked) {
                          setSelectedInterfaces([...selectedInterfaces, ui]);
                        } else {
                          setSelectedInterfaces(sel => sel.filter(i => i !== ui));
                        }
                      }}
                    />
                  }
                  label={
                    <Box>
                      <Typography variant="body1" fontWeight="bold">{ui}</Typography>
                      <Typography variant="body2" color="text.secondary">
                        Devices: {getDevicesForInterface(ui).join(', ')}
                      </Typography>
                      <Typography variant="caption" color="success.main">
                        ‚úÖ {analysis.compatibility_matrix.reasons[ui]}
                      </Typography>
                    </Box>
                  }
                />
              </Card>
            ))}
          </Box>

          {analysis.compatibility_matrix.incompatible?.length > 0 && (
            <Alert severity="warning" sx={{ mb: 2 }}>
              <Typography variant="subtitle2">Incompatible Interfaces:</Typography>
              <ul>
                {analysis.compatibility_matrix.incompatible.map(ui => (
                  <li key={ui}>
                    <strong>{ui}</strong>: {analysis.compatibility_matrix.reasons[ui]}
                  </li>
                ))}
              </ul>
            </Alert>
          )}

          <Box sx={{ display: 'flex', justifyContent: 'space-between' }}>
            <Button onClick={() => setStep('input')}>Back</Button>
            <Button 
              variant="contained"
              onClick={async () => {
                setStep('generation');
                const testCases = await generateTestCases(
                  analysis.analysis_id, 
                  selectedInterfaces
                );
                onTestCasesCreated(testCases);
              }}
              disabled={selectedInterfaces.length === 0}
            >
              Generate {selectedInterfaces.length} Test Case{selectedInterfaces.length !== 1 ? 's' : ''}
            </Button>
          </Box>
        </Box>
      )}

      {/* STEP 3: Generation Progress */}
      {step === 'generation' && (
        <Box sx={{ textAlign: 'center', py: 6 }}>
          <CircularProgress size={60} sx={{ mb: 2 }} />
          <Typography variant="h6">Generating test cases...</Typography>
          <Typography variant="body2" color="text.secondary">
            Creating {selectedInterfaces.length} executable test case{selectedInterfaces.length !== 1 ? 's' : ''} 
            for selected interfaces
          </Typography>
        </Box>
      )}
    </Box>
  );
}
```

### 3. Custom Hook (`frontend/src/hooks/aiagent/useAITestCase.ts`)

```tsx
export function useAITestCase() {
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const analyzeTestCase = async (prompt: string) => {
    setLoading(true);
    setError(null);
    try {
      const response = await fetch('/server/analyzeTestCase', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ prompt })
      });
      
      if (!response.ok) {
        throw new Error(`Analysis failed: ${response.statusText}`);
      }
      
      return await response.json();
    } catch (err) {
      setError(err.message);
      throw err;
    } finally {
      setLoading(false);
    }
  };

  const generateTestCases = async (analysisId: string, confirmedInterfaces: string[]) => {
    setLoading(true);
    setError(null);
    try {
      const response = await fetch('/server/generateTestCases', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ 
          analysis_id: analysisId,
          confirmed_userinterfaces: confirmedInterfaces 
        })
      });
      
      if (!response.ok) {
        throw new Error(`Generation failed: ${response.statusText}`);
      }
      
      const result = await response.json();
      return result.generated_testcases || [];
    } catch (err) {
      setError(err.message);
      throw err;
    } finally {
      setLoading(false);
    }
  };

  return { loading, error, analyzeTestCase, generateTestCases };
}
```

### 4. Enhanced Types (`frontend/src/types/pages/TestCase_Types.ts`)

```typescript
// Extend existing TestCase interface
export interface TestCase {
  // Existing fields...
  id: string;
  name: string;
  description: string;
  steps: TestStep[];
  
  // NEW: AI-specific fields
  creator: 'manual' | 'ai';
  original_prompt?: string;
  ai_analysis?: AIAnalysis;
  compatible_devices?: string[];
  compatible_userinterfaces?: string[];
  device_adaptations?: Record<string, any>;
}

export interface AIAnalysis {
  analysis_id: string;
  understanding: string;
  compatibility_matrix: {
    compatible_userinterfaces: string[];
    compatible_devices: string[];
    incompatible: string[];
    reasons: Record<string, string>;
  };
  requires_multiple_testcases: boolean;
  estimated_complexity: 'low' | 'medium' | 'high';
}

export interface AITestCaseRequest {
  prompt: string;
}

export interface AIGenerationRequest {
  analysis_id: string;
  confirmed_userinterfaces: string[];
}
```

---

## üîß Backend Implementation

### 1. Server Routes (`backend_server/src/routes/server_aitestcase_routes.py`)

```python
from flask import Blueprint, request, jsonify
import uuid
from backend_host.src.controllers.ai.ai_agent import AIAgentController
from shared.lib.supabase.testcase_db import save_test_case, get_test_case
from shared.lib.supabase.userinterface_db import get_all_userinterfaces, get_userinterface_by_name
from shared.lib.supabase.navigation_trees_db import get_full_tree
from shared.lib.utils.app_utils import get_team_id
from shared.lib.utils.route_utils import proxy_to_host

bp = Blueprint('server_aitestcase', __name__)

@bp.route('/analyzeTestCase', methods=['POST'])
def analyze_test_case():
    """
    STEP 1: Analyze test case compatibility against ALL userinterfaces
    
    Input: { "prompt": "Go to live and check audio" }
    
    Output: { 
        "analysis_id": "uuid",
        "understanding": "Navigate to live TV and verify audio functionality",
        "compatibility_matrix": {
            "compatible_userinterfaces": ["horizon_android_mobile", "horizon_android_tv"],
            "compatible_devices": ["samsung_galaxy", "nvidia_shield"],
            "incompatible": ["web_interface"],
            "reasons": {"horizon_android_mobile": "Has required navigation and audio verification"}
        },
        "requires_multiple_testcases": false,
        "estimated_complexity": "medium"
    }
    """
    try:
        team_id = get_team_id()
        request_data = request.get_json() or {}
        prompt = request_data.get('prompt')
        
        if not prompt:
            return jsonify({'success': False, 'error': 'Prompt is required'}), 400
        
        # Get all available userinterfaces for this team
        userinterfaces = get_all_userinterfaces(team_id)
        
        if not userinterfaces:
            return jsonify({
                'success': False, 
                'error': 'No userinterfaces found for analysis'
            }), 404
        
        # Initialize AI agent
        ai_agent = AIAgentController()
        
        # Analyze compatibility with each userinterface
        compatibility_results = []
        
        for ui in userinterfaces:
            try:
                # Get navigation graph for this interface
                navigation_graph = get_full_tree(ui['name'], team_id)
                
                # Prepare analysis context
                analysis_context = {
                    'prompt': prompt,
                    'userinterface_name': ui['name'],
                    'navigation_nodes': list(navigation_graph.keys()) if navigation_graph else [],
                    'available_actions': ['click_element', 'navigate', 'wait', 'press_key'],
                    'available_verifications': ['verify_image', 'verify_audio', 'verify_video', 'verify_text']
                }
                
                # AI compatibility analysis
                compatibility = ai_agent.execute_task(
                    f"Analyze if this test case '{prompt}' is feasible for userinterface '{ui['name']}'",
                    analysis_context
                )
                
                compatibility_results.append({
                    'userinterface': ui['name'],
                    'compatible': compatibility.get('feasible', False),
                    'reasoning': compatibility.get('reasoning', 'Analysis unavailable'),
                    'confidence': compatibility.get('confidence', 0.5),
                    'missing_capabilities': compatibility.get('missing_capabilities', [])
                })
                
            except Exception as e:
                print(f"Error analyzing {ui['name']}: {e}")
                compatibility_results.append({
                    'userinterface': ui['name'],
                    'compatible': False,
                    'reasoning': f'Analysis failed: {str(e)}',
                    'confidence': 0.0
                })
        
        # Separate compatible and incompatible interfaces
        compatible = [r for r in compatibility_results if r['compatible']]
        incompatible = [r for r in compatibility_results if not r['compatible']]
        
        # Generate unique analysis ID
        analysis_id = str(uuid.uuid4())
        
        # Build analysis result
        analysis_result = {
            'analysis_id': analysis_id,
            'understanding': f"Test case analysis: {prompt}",
            'compatibility_matrix': {
                'compatible_userinterfaces': [ui['userinterface'] for ui in compatible],
                'incompatible': [ui['userinterface'] for ui in incompatible],
                'reasons': {ui['userinterface']: ui['reasoning'] for ui in compatibility_results}
            },
            'requires_multiple_testcases': len(compatible) > 1,
            'estimated_complexity': 'medium',
            'total_analyzed': len(userinterfaces),
            'compatible_count': len(compatible)
        }
        
        # Cache analysis result (implement cache table save)
        # save_analysis_cache(analysis_id, prompt, analysis_result, team_id)
        
        return jsonify(analysis_result)
        
    except Exception as e:
        print(f"Analysis error: {e}")
        return jsonify({
            'success': False,
            'error': f'Analysis failed: {str(e)}'
        }), 500

@bp.route('/generateTestCases', methods=['POST'])
def generate_test_cases():
    """
    STEP 2: Generate actual test cases for confirmed userinterfaces
    
    Input: { 
        "analysis_id": "uuid",
        "confirmed_userinterfaces": ["horizon_android_mobile", "horizon_android_tv"]
    }
    
    Output: {
        "success": true,
        "generated_testcases": [
            {
                "id": "uuid",
                "name": "Go to live and check audio - Android Mobile",
                "description": "Navigate to live TV and verify audio functionality on mobile interface",
                "creator": "ai",
                "original_prompt": "Go to live and check audio",
                "steps": [...],
                "compatible_userinterfaces": ["horizon_android_mobile"]
            }
        ]
    }
    """
    try:
        team_id = get_team_id()
        request_data = request.get_json() or {}
        analysis_id = request_data.get('analysis_id')
        confirmed_interfaces = request_data.get('confirmed_userinterfaces', [])
        
        if not analysis_id or not confirmed_interfaces:
            return jsonify({
                'success': False,
                'error': 'analysis_id and confirmed_userinterfaces are required'
            }), 400
        
        # For now, we'll reconstruct the prompt (in future, retrieve from cache)
        # original_prompt = get_analysis_cache(analysis_id, team_id)['prompt']
        # For demo, we'll use a placeholder
        original_prompt = "Generated test case"  # TODO: Get from cache
        
        # Generate test cases for each confirmed interface
        ai_agent = AIAgentController()
        generated_testcases = []
        
        for interface_name in confirmed_interfaces:
            try:
                # Get interface data and navigation graph
                interface_data = get_userinterface_by_name(interface_name, team_id)
                navigation_graph = get_full_tree(interface_name, team_id)
                
                # Prepare generation context
                generation_context = {
                    'prompt': original_prompt,
                    'userinterface_name': interface_name,
                    'navigation_graph': navigation_graph,
                    'interface_data': interface_data,
                    'generate_executable_steps': True
                }
                
                # Generate specific test case
                test_case_result = ai_agent.execute_task(
                    f"Generate executable test case steps for: {original_prompt}",
                    generation_context
                )
                
                if test_case_result.get('success', True):  # Assume success if not specified
                    # Create test case object
                    test_case = {
                        'name': f"{original_prompt} - {interface_name}",
                        'description': test_case_result.get('description', f"AI-generated test case for {interface_name}"),
                        'creator': 'ai',
                        'original_prompt': original_prompt,
                        'ai_analysis': {'analysis_id': analysis_id},
                        'compatible_userinterfaces': [interface_name],
                        'steps': test_case_result.get('steps', []),
                        'verifications': test_case_result.get('verifications', []),
                        'status': 'ready'
                    }
                    
                    # Save to database
                    saved_test_case = save_test_case(test_case, team_id)
                    generated_testcases.append(saved_test_case)
                    
                else:
                    print(f"Generation failed for {interface_name}: {test_case_result.get('error')}")
                    
            except Exception as e:
                print(f"Error generating test case for {interface_name}: {e}")
        
        return jsonify({
            'success': True,
            'generated_testcases': generated_testcases,
            'total_generated': len(generated_testcases)
        })
        
    except Exception as e:
        print(f"Generation error: {e}")
        return jsonify({
            'success': False,
            'error': f'Generation failed: {str(e)}'
        }), 500

@bp.route('/executeTestCase', methods=['POST'])
def execute_test_case():
    """Execute test case - PROXY TO HOST"""
    try:
        team_id = get_team_id()
        request_data = request.get_json() or {}
        test_case_id = request_data.get('test_case_id')
        device_id = request_data.get('device_id')
        
        if not all([test_case_id, device_id]):
            return jsonify({
                'success': False,
                'error': 'test_case_id and device_id are required'
            }), 400
        
        # Get test case from database
        test_case = get_test_case(test_case_id, team_id)
        
        if not test_case:
            return jsonify({
                'success': False,
                'error': 'Test case not found'
            }), 404
        
        # Proxy to host for execution
        host_request = {
            'test_case': test_case,
            'device_id': device_id,
            'team_id': team_id
        }
        
        response_data, status_code = proxy_to_host(
            '/host/executeTestCase',
            'POST',
            host_request
        )
        
        return jsonify(response_data), status_code
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Execution proxy failed: {str(e)}'
        }), 500
```

### 2. Host Routes (`backend_host/src/routes/host_aitestcase_routes.py`)

```python
from flask import Blueprint, request, jsonify
from shared.lib.utils.script_execution_utils import execute_script
from shared.lib.utils.script_framework import ScriptExecutor

bp = Blueprint('host_aitestcase', __name__)

@bp.route('/executeTestCase', methods=['POST'])
def execute_test_case():
    """
    Execute AI-generated test case on device
    
    Input: {
        "test_case": {...},
        "device_id": "device123",
        "team_id": "team456"
    }
    
    Output: {
        "success": true,
        "execution_results": {...},
        "test_case_name": "Go to live and check audio - Android Mobile"
    }
    """
    try:
        request_data = request.get_json() or {}
        test_case = request_data.get('test_case')
        device_id = request_data.get('device_id')
        team_id = request_data.get('team_id')
        
        if not all([test_case, device_id, team_id]):
            return jsonify({
                'success': False,
                'error': 'test_case, device_id, and team_id are required'
            }), 400
        
        print(f"[@host_aitestcase:execute] Executing AI test case: {test_case.get('name', 'Unknown')}")
        
        # Convert AI test case to script execution format
        script_data = {
            'name': test_case['name'],
            'description': test_case.get('description', ''),
            'steps': test_case.get('steps', []),
            'verifications': test_case.get('verifications', []),
            'creator': 'ai',
            'original_prompt': test_case.get('original_prompt', ''),
            'userinterface_name': test_case.get('compatible_userinterfaces', [None])[0]
        }
        
        # Execute using existing script execution framework
        executor = ScriptExecutor("ai_testcase", test_case['name'])
        execution_result = execute_script(executor, script_data, device_id, team_id)
        
        return jsonify({
            'success': execution_result.get('success', False),
            'execution_results': execution_result,
            'test_case_name': test_case['name'],
            'device_id': device_id,
            'creator': 'ai'
        })
        
    except Exception as e:
        print(f"[@host_aitestcase:execute] Error: {e}")
        return jsonify({
            'success': False,
            'error': f'Test case execution failed: {str(e)}'
        }), 500
```

---

## üìã Detailed Workflow

### Complete User Journey

1. **TestCase Page Load**
   - User sees existing test cases table
   - "Creator" column shows AI vs Manual badges
   - "Create Test Case" button visible

2. **Dialog Opens - Input Step**
   - User clicks "Create Test Case"
   - Dialog opens with prompt textarea
   - User types: "Go to live and check audio"
   - Clicks "Analyze Compatibility"

3. **Analysis Step**
   - AI analyzes prompt against ALL userinterfaces
   - Returns compatibility matrix
   - Shows compatible interfaces with checkboxes
   - Shows incompatible interfaces with reasons
   - User selects desired interfaces

4. **Generation Step**
   - Shows generation progress
   - AI creates individual test case for each selected interface
   - Test cases saved to database
   - Dialog closes automatically

5. **Table Refresh**
   - New test cases appear in table
   - Each shows "AI" creator badge
   - Original prompt visible in details
   - Ready for execution

### Backend Flow

```
POST /server/analyzeTestCase
‚îú‚îÄ‚îÄ Get all userinterfaces for team
‚îú‚îÄ‚îÄ For each interface:
‚îÇ   ‚îú‚îÄ‚îÄ Get navigation graph
‚îÇ   ‚îú‚îÄ‚îÄ AI analysis of feasibility
‚îÇ   ‚îî‚îÄ‚îÄ Build compatibility result
‚îú‚îÄ‚îÄ Generate analysis_id
‚îú‚îÄ‚îÄ Cache result (future)
‚îî‚îÄ‚îÄ Return compatibility matrix

POST /server/generateTestCases  
‚îú‚îÄ‚îÄ Get confirmed interfaces
‚îú‚îÄ‚îÄ For each interface:
‚îÇ   ‚îú‚îÄ‚îÄ Get interface data & navigation
‚îÇ   ‚îú‚îÄ‚îÄ AI generate executable steps
‚îÇ   ‚îú‚îÄ‚îÄ Create test case object
‚îÇ   ‚îî‚îÄ‚îÄ Save to database
‚îî‚îÄ‚îÄ Return generated test cases

POST /server/executeTestCase
‚îú‚îÄ‚îÄ Get test case from database
‚îú‚îÄ‚îÄ Proxy to host with test case data
‚îî‚îÄ‚îÄ Return execution results
```

---

## üöÄ Implementation Phases

### Phase 1: Database & Backend Core
1. **Database Schema Updates**
   - Add new columns to `test_cases` table
   - Create `ai_analysis_cache` table
   - Update indexes

2. **Backend Routes**
   - ‚úÖ Create `server_aitestcase_routes.py`
   - ‚úÖ Create `host_aitestcase_routes.py`
   - Register route blueprints

3. **AI Agent Enhancement**
   - Add compatibility analysis methods
   - Add test case generation logic
   - Integration with navigation graphs

### Phase 2: Frontend Implementation
1. **Types & Hooks**
   - ‚úÖ Update `TestCase_Types.ts`
   - ‚úÖ Create `useAITestCase.ts` hook

2. **Components**
   - ‚úÖ Create `AITestCaseGenerator.tsx`
   - Update `TestCaseEditor.tsx` with dialog
   - Add creator column to table

3. **UI Polish**
   - Compatibility matrix display
   - Progress indicators
   - Error handling

### Phase 3: Integration & Testing
1. **End-to-End Testing**
   - Test various prompts
   - Validate compatibility analysis
   - Test generation accuracy

2. **UI/UX Refinement**
   - Responsive design
   - Loading states
   - Error messages

---

## üéØ Success Criteria

### Functional Requirements
- ‚úÖ **Main page preserves existing table** functionality
- ‚úÖ **Dialog provides two-step AI generation** 
- ‚úÖ **AI analyzes ALL userinterfaces** for compatibility
- ‚úÖ **Individual test cases generated** per interface
- ‚úÖ **Creator distinction clear** (AI vs Manual)
- ‚úÖ **Execution works** through existing framework

### Technical Requirements  
- ‚úÖ **Clean modern code** - no fallbacks/legacy
- ‚úÖ **Proper database schema** with new columns
- ‚úÖ **Server generates, Host executes** architecture
- ‚úÖ **Existing patterns followed** for routes

### User Experience
- ‚úÖ **Intuitive workflow** - Input ‚Üí Analysis ‚Üí Generation
- ‚úÖ **Clear compatibility feedback**
- ‚úÖ **Seamless integration** with existing UI
- ‚úÖ **Fast response times** for analysis

---

## üìù Next Steps

1. **Update database schema** with new columns
2. **Enhance AI agent** with analysis capabilities  
3. **Complete backend routes** with real AI integration
4. **Build frontend dialog** with two-step flow
5. **End-to-end testing** with real prompts

This plan provides a complete roadmap for implementing AI test case generation while maintaining the existing UI structure and adding powerful AI capabilities through a clean, modern dialog-based interface.
